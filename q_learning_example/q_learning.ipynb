{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73460a26",
   "metadata": {},
   "source": [
    "# Q-Learning Implementation for Gridworld Navigation\n",
    "\n",
    "This notebook demonstrates a practical implementation of Q-learning, a fundamental reinforcement learning algorithm, applied to a gridworld navigation problem.\n",
    "\n",
    "## Introduction to Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions and learns to maximize cumulative rewards over time.\n",
    "\n",
    "**Key Components:**\n",
    "- **Agent**: The decision-maker (our navigator)\n",
    "- **Environment**: The world the agent operates in (5×5 grid)\n",
    "- **State**: Current situation of the agent (position on grid)\n",
    "- **Action**: What the agent can do (move up, down, left, right)\n",
    "- **Reward**: Feedback from the environment (+1 for goal, -1 for obstacle, 0 otherwise)\n",
    "- **Policy**: Strategy for choosing actions\n",
    "\n",
    "## Problem Setup: Gridworld Navigation\n",
    "\n",
    "**Objective**: Navigate from start position to goal position while avoiding obstacles.\n",
    "\n",
    "**Grid Layout:**\n",
    "- **5×5 grid** environment\n",
    "- **Start**: Top-left corner (0,0)\n",
    "- **Goal**: Bottom-right corner (4,4) → Reward: +1\n",
    "- **Obstacle**: Center position (2,2) → Penalty: -1\n",
    "- **Empty spaces**: Reward: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4efdc8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use:\n",
    "- **NumPy**: For numerical operations and Q-table management\n",
    "- **Matplotlib**: For plotting and visualization\n",
    "- **Seaborn**: For enhanced heatmap visualization\n",
    "- **Random**: For implementing epsilon-greedy exploration strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1693db35",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Q-Learning Parameters\n",
    "\n",
    "### Environment Definition:\n",
    "- **Grid Size**: 5×5 discrete state space\n",
    "- **States**: Each cell position (i,j) represents a state\n",
    "- **Actions**: 4 possible moves (up, down, left, right)\n",
    "- **Rewards**: Sparse reward system focusing agent on goal-seeking behavior\n",
    "\n",
    "### Q-Table Structure:\n",
    "The Q-table is a 3D array with dimensions `[grid_size, grid_size, num_actions]` where:\n",
    "- First two dimensions represent the state (position)\n",
    "- Third dimension represents the action values for that state\n",
    "\n",
    "### Hyperparameters:\n",
    "- **α (alpha)**: Learning rate (0.1) - How much new information overrides old\n",
    "- **γ (gamma)**: Discount factor (0.9) - Importance of future rewards vs immediate rewards\n",
    "- **ε (epsilon)**: Exploration rate (0.1) - Probability of taking random action vs best known action\n",
    "- **Episodes**: Number of training iterations (500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "grid_size = 5\n",
    "goal_state = (4, 4)\n",
    "obstacle_state = (2, 2)\n",
    "start_state = (0, 0)\n",
    "\n",
    "# Rewards: +1 for goal, -1 for obstacle, 0 for everything else\n",
    "rewards = np.zeros((grid_size, grid_size))\n",
    "rewards[goal_state] = 1\n",
    "rewards[obstacle_state] = -1\n",
    "\n",
    "# Actions: up, down, left, right\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_mapping = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "action_arrows = {\n",
    "    'up': (0, 0.3),\n",
    "    'down': (0, -0.3),\n",
    "    'left': (-0.3, 0),\n",
    "    'right': (0.3, 0)\n",
    "}\n",
    "\n",
    "# Initialize Q-values table\n",
    "Q_values = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "episodes = 500  # Number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48eb273",
   "metadata": {},
   "source": [
    "## 7. Analysis and Key Insights\n",
    "\n",
    "### What the Agent Learned:\n",
    "\n",
    "1. **Optimal Navigation**: The agent discovered the shortest path from start to goal\n",
    "2. **Obstacle Avoidance**: Learned to navigate around the obstacle without being explicitly programmed\n",
    "3. **Value Propagation**: High values near the goal propagated backward through the state space\n",
    "4. **Policy Convergence**: Actions converged to an optimal policy for each state\n",
    "\n",
    "### Hyperparameter Effects:\n",
    "\n",
    "**Learning Rate (α):**\n",
    "- **Higher α**: Faster learning but potentially unstable\n",
    "- **Lower α**: More stable but slower convergence\n",
    "\n",
    "**Discount Factor (γ):**\n",
    "- **Higher γ**: Values future rewards more, leads to longer-term planning\n",
    "- **Lower γ**: Focuses on immediate rewards, more myopic behavior\n",
    "\n",
    "**Exploration Rate (ε):**\n",
    "- **Higher ε**: More exploration, slower convergence but better final policy\n",
    "- **Lower ε**: Less exploration, faster convergence but risk of suboptimal policy\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "- **Robotics**: Path planning and navigation\n",
    "- **Game AI**: Strategic decision making\n",
    "- **Finance**: Trading strategies and portfolio optimization\n",
    "- **Resource Management**: Optimal allocation and scheduling\n",
    "- **Autonomous Vehicles**: Route planning and traffic navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Exercises and Extensions\n",
    "\n",
    "### Try These Modifications:\n",
    "\n",
    "1. **Change the Environment:**\n",
    "   - Add more obstacles in different positions\n",
    "   - Create a larger grid (e.g., 10×10)\n",
    "   - Add multiple goals with different rewards\n",
    "   - Create walls that the agent cannot pass through\n",
    "\n",
    "2. **Experiment with Hyperparameters:**\n",
    "   - **Learning Rate**: Try α = 0.01, 0.5, 0.9\n",
    "   - **Discount Factor**: Try γ = 0.1, 0.99\n",
    "   - **Exploration**: Try ε = 0.01, 0.3, implement ε-decay\n",
    "   - **Episodes**: Train for 1000 or 5000 episodes\n",
    "\n",
    "3. **Advanced Features:**\n",
    "   - **ε-decay**: Start with high exploration, decrease over time\n",
    "   - **Learning curves**: Plot cumulative reward vs episodes\n",
    "   - **Multiple runs**: Average results over multiple random seeds\n",
    "   - **Stochastic environment**: Add randomness to transitions\n",
    "\n",
    "4. **Different Reward Structures:**\n",
    "   - **Living penalty**: -0.01 for each step (encourages shorter paths)\n",
    "   - **Distance-based rewards**: Reward inversely proportional to distance from goal\n",
    "   - **Sparse vs Dense**: Compare current sparse rewards with dense reward shaping\n",
    "\n",
    "### Advanced Q-Learning Variants:\n",
    "\n",
    "- **Double Q-Learning**: Reduces overestimation bias\n",
    "- **Deep Q-Networks (DQN)**: Use neural networks for large state spaces\n",
    "- **SARSA**: On-policy alternative to Q-learning\n",
    "- **Expected SARSA**: Combines benefits of Q-learning and SARSA\n",
    "\n",
    "### Performance Metrics to Track:\n",
    "\n",
    "- **Convergence Speed**: Episodes needed to reach optimal policy\n",
    "- **Sample Efficiency**: How many experiences needed for good performance\n",
    "- **Final Performance**: Success rate and path length after training\n",
    "- **Robustness**: Performance across different random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state = start_state\n",
    "    \n",
    "    while state != goal_state:\n",
    "        action = choose_action(state)\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = rewards[next_state]\n",
    "        \n",
    "        # Q-learning update\n",
    "        current_q_value = Q_values[state[0], state[1], actions.index(action)]\n",
    "        max_future_q_value = np.max(Q_values[next_state[0], next_state[1], :])\n",
    "        \n",
    "        new_q_value = current_q_value + alpha * (reward + gamma * max_future_q_value - current_q_value)\n",
    "        Q_values[state[0], state[1], actions.index(action)] = new_q_value\n",
    "        \n",
    "        # Move to next state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa44ff1",
   "metadata": {},
   "source": [
    "## 6. Visualization of Learned Policy\n",
    "\n",
    "Now we'll visualize what the agent has learned through the training process.\n",
    "\n",
    "### State Value Visualization:\n",
    "The heatmap shows the **maximum Q-value** for each state, representing how \"valuable\" each position is:\n",
    "- **Warm colors (red/orange)**: High-value states (closer to goal)\n",
    "- **Cool colors (blue)**: Low-value states (farther from goal or near obstacles)\n",
    "\n",
    "### Policy Arrows:\n",
    "Black arrows indicate the **optimal action** for each state:\n",
    "- Each arrow points in the direction the agent should move from that position\n",
    "- This represents the learned policy (strategy) for navigation\n",
    "\n",
    "### Expected Results:\n",
    "- **Goal state** should have the highest value\n",
    "- **Obstacle state** should have low/negative value\n",
    "- **Arrows should point toward the goal** while avoiding the obstacle\n",
    "- **Clear path** from start to goal should emerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of learned Q-values\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Sum Q-values for each state to visualize overall state values\n",
    "state_values = np.max(Q_values, axis=2)\n",
    "\n",
    "# Plot heatmap of state values\n",
    "sns.heatmap(state_values, annot=True, cmap='coolwarm', cbar=True, fmt=\".2f\", ax=ax)\n",
    "\n",
    "# Highlight the goal and obstacle\n",
    "ax.add_patch(plt.Rectangle(goal_state, 1, 1, fill=False, edgecolor='green', lw=3))\n",
    "ax.add_patch(plt.Rectangle(obstacle_state, 1, 1, fill=False, edgecolor='red', lw=3))\n",
    "\n",
    "# Adding arrows for the learned policy\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if (i, j) != goal_state and (i, j) != obstacle_state:\n",
    "            best_action_idx = np.argmax(Q_values[i, j, :])\n",
    "            best_action = actions[best_action_idx]\n",
    "            arrow = action_arrows[best_action]\n",
    "            ax.arrow(j + 0.5, i + 0.5, arrow[0], -arrow[1], head_width=0.2, head_length=0.2, fc='black', ec='black')\n",
    "\n",
    "plt.title('Q-values Heatmap with Optimal Policy Arrows')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873b966",
   "metadata": {},
   "source": [
    "## 5. Q-Learning Algorithm Implementation\n",
    "\n",
    "This is the core of our reinforcement learning agent. The algorithm learns optimal actions through trial and error.\n",
    "\n",
    "### Q-Learning Update Rule:\n",
    "The fundamental equation that drives learning:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]$$\n",
    "\n",
    "Where:\n",
    "- **Q(s,a)**: Current Q-value for state s and action a\n",
    "- **α**: Learning rate (how fast we update our beliefs)\n",
    "- **r**: Immediate reward received\n",
    "- **γ**: Discount factor (how much we value future rewards)\n",
    "- **max Q(s',a')**: Maximum Q-value for the next state (best possible future value)\n",
    "\n",
    "### Algorithm Steps:\n",
    "1. **Start** each episode at the starting position\n",
    "2. **Choose action** using epsilon-greedy policy\n",
    "3. **Execute action** and observe next state and reward\n",
    "4. **Update Q-value** using the Q-learning formula\n",
    "5. **Move** to next state\n",
    "6. **Repeat** until goal is reached\n",
    "7. **Start new episode**\n",
    "\n",
    "### Learning Process:\n",
    "- Initially, all Q-values are zero (no knowledge)\n",
    "- Through exploration, the agent discovers rewards and penalties\n",
    "- Q-values gradually converge to optimal values\n",
    "- The agent learns the shortest path to the goal while avoiding obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffdb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "553eb993",
   "metadata": {},
   "source": [
    "## 4. Epsilon-Greedy Action Selection\n",
    "\n",
    "This function implements the **exploration vs exploitation** trade-off, which is crucial for effective learning in reinforcement learning.\n",
    "\n",
    "### The Exploration-Exploitation Dilemma:\n",
    "- **Exploitation**: Choose the action with the highest Q-value (greedy)\n",
    "- **Exploration**: Choose a random action to discover potentially better strategies\n",
    "\n",
    "### Epsilon-Greedy Strategy:\n",
    "- With probability **ε**: Take a **random action** (explore)\n",
    "- With probability **(1-ε)**: Take the **best known action** (exploit)\n",
    "\n",
    "**Benefits:**\n",
    "- Ensures the agent doesn't get stuck in local optima\n",
    "- Balances learning new strategies with using current knowledge\n",
    "- ε can be decayed over time (start high, reduce as learning progresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39507b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e67bb95d",
   "metadata": {},
   "source": [
    "## 3. State Transition Function\n",
    "\n",
    "This function handles the environment dynamics - how the agent moves within the grid world.\n",
    "\n",
    "**Key Features:**\n",
    "- **Boundary Checking**: Prevents agent from moving outside the grid\n",
    "- **Deterministic Transitions**: Each action always produces the same result\n",
    "- **Simple Physics**: Agent moves one cell at a time\n",
    "\n",
    "**Input**: Current state and chosen action  \n",
    "**Output**: Next state after applying the action"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
