{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07576166",
   "metadata": {},
   "source": [
    "# Autoencoder for Anomaly Detection in Energy Systems\n",
    "\n",
    "This notebook demonstrates the implementation of a denoising autoencoder for anomaly detection in energy system data. Autoencoders are powerful neural networks that can learn to compress and reconstruct data, making them excellent tools for identifying anomalies and denoising signals.\n",
    "\n",
    "## Introduction to Autoencoders\n",
    "\n",
    "An **autoencoder** is a type of neural network designed to learn efficient representations of data by compressing it into a lower-dimensional space (encoding) and then reconstructing it back to the original dimensions (decoding).\n",
    "\n",
    "### Key Components:\n",
    "- **Encoder**: Compresses input data into a latent representation\n",
    "- **Bottleneck/Latent Space**: Compressed representation of the input\n",
    "- **Decoder**: Reconstructs the original data from the latent representation\n",
    "\n",
    "### Applications in Energy Systems:\n",
    "- **Anomaly Detection**: Identifying unusual patterns in power consumption, equipment failures\n",
    "- **Signal Denoising**: Cleaning sensor data from electrical noise and interference\n",
    "- **Predictive Maintenance**: Detecting early signs of equipment degradation\n",
    "- **Data Compression**: Efficient storage and transmission of sensor data\n",
    "- **Quality Control**: Identifying faulty measurements or sensor malfunctions\n",
    "\n",
    "## Problem Setup: Denoising Energy System Signals\n",
    "\n",
    "**Objective**: Train an autoencoder to remove noise from complex energy system signals while preserving important patterns.\n",
    "\n",
    "**Challenge**: Energy systems produce complex, multi-frequency signals that can be corrupted by:\n",
    "- Electrical interference\n",
    "- Sensor noise\n",
    "- Environmental factors\n",
    "- Equipment vibrations\n",
    "- Communication errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010bf03",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use the following libraries:\n",
    "- **PyTorch**: Deep learning framework for building and training the autoencoder\n",
    "- **NumPy**: Numerical operations and data generation\n",
    "- **Matplotlib**: Visualization of results\n",
    "- **scikit-learn**: Data splitting utilities\n",
    "\n",
    "PyTorch is particularly well-suited for this task because:\n",
    "- Automatic differentiation for gradient computation\n",
    "- Flexible neural network building blocks\n",
    "- GPU acceleration capabilities\n",
    "- Extensive optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6f3fd",
   "metadata": {},
   "source": [
    "## 2. Set Random Seeds for Reproducibility\n",
    "\n",
    "Setting random seeds ensures that:\n",
    "- **Consistent Results**: Same output across different runs\n",
    "- **Debugging**: Easier to identify and fix issues\n",
    "- **Comparison**: Fair evaluation of different approaches\n",
    "- **Research**: Reproducible experiments for scientific validity\n",
    "\n",
    "This is crucial for machine learning experiments where randomness affects:\n",
    "- Weight initialization\n",
    "- Data shuffling\n",
    "- Dropout layers\n",
    "- Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(50)\n",
    "np.random.seed(50)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc54056",
   "metadata": {},
   "source": [
    "## 3. Generate Complex Synthetic Energy System Data\n",
    "\n",
    "Since real energy system data may be proprietary or sensitive, we'll create realistic synthetic data that mimics common characteristics of energy system signals.\n",
    "\n",
    "### Signal Characteristics:\n",
    "Our synthetic data simulates typical energy system signals with:\n",
    "\n",
    "1. **Multi-frequency Components**:\n",
    "   - **Fundamental frequency**: Main sinusoidal component (50/60 Hz power systems)\n",
    "   - **Harmonics**: Higher frequency components (2x, 4x fundamental)\n",
    "   - **Phase variations**: Random phase shifts simulating different operating conditions\n",
    "\n",
    "2. **Complex Noise Model**:\n",
    "   - **Gaussian noise**: Random measurement uncertainty\n",
    "   - **Spike noise**: Sudden electrical disturbances or switching events\n",
    "   - **Uniform noise**: Background electrical interference\n",
    "\n",
    "### Why This Approach?\n",
    "- **Realistic**: Captures real-world signal complexity\n",
    "- **Controlled**: We know the ground truth for evaluation\n",
    "- **Scalable**: Can easily modify parameters for different scenarios\n",
    "- **Educational**: Demonstrates how autoencoders handle various noise types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Complex Synthetic Dataset\n",
    "def generate_complex_sequence(length=50, num_sequences=1000, noise_factor=0.5):\n",
    "    x = np.linspace(0, 4 * np.pi, length)\n",
    "    \n",
    "    # Create complex clean signal by combining sinusoids of different frequencies\n",
    "    clean_sequences = np.array([\n",
    "        np.sin(x + np.random.uniform(0, 2 * np.pi)) + \n",
    "        0.5 * np.sin(2 * x + np.random.uniform(0, 2 * np.pi)) + \n",
    "        0.25 * np.sin(4 * x + np.random.uniform(0, 2 * np.pi)) \n",
    "        for _ in range(num_sequences)\n",
    "    ])\n",
    "    \n",
    "    # Add complex noise: Gaussian noise + occasional spikes + uniform noise\n",
    "    gaussian_noise = noise_factor * np.random.normal(size=clean_sequences.shape)\n",
    "    spike_noise = np.random.choice([0, 1], size=clean_sequences.shape, p=[0.98, 0.02]) * np.random.uniform(-3, 3, size=clean_sequences.shape)\n",
    "    uniform_noise = noise_factor * np.random.uniform(-1, 1, size=clean_sequences.shape)\n",
    "    \n",
    "    noisy_sequences = clean_sequences + gaussian_noise + spike_noise + uniform_noise\n",
    "    return torch.tensor(noisy_sequences, dtype=torch.float32), torch.tensor(clean_sequences, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c7156",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Train-Test Split\n",
    "\n",
    "### Dataset Creation:\n",
    "- **1000 sequences** of length 50 time steps each\n",
    "- **Training set**: 80% of the data (800 sequences)\n",
    "- **Test set**: 20% of the data (200 sequences)\n",
    "\n",
    "### Training Strategy:\n",
    "The autoencoder will be trained to:\n",
    "- **Input**: Noisy sequences (what sensors actually measure)\n",
    "- **Target**: Clean sequences (ideal signal without noise)\n",
    "- **Learn**: Mapping from noisy → clean (denoising function)\n",
    "\n",
    "This supervised learning approach allows the autoencoder to learn robust features that distinguish between signal and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4062b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "noisy_data, clean_data = generate_complex_sequence()\n",
    "train_noisy, test_noisy, train_clean, test_clean = train_test_split(noisy_data, clean_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c280e7",
   "metadata": {},
   "source": [
    "## 5. Autoencoder Architecture Design\n",
    "\n",
    "### Network Structure:\n",
    "Our denoising autoencoder follows a symmetric encoder-decoder architecture:\n",
    "\n",
    "**Encoder Path:**\n",
    "1. **Input Layer**: 50 time steps → Hidden Layer (64 neurons)\n",
    "2. **Hidden Layer**: 64 neurons → Bottleneck (32 neurons)\n",
    "3. **Activation**: ReLU (Rectified Linear Unit) for non-linearity\n",
    "\n",
    "**Decoder Path:**\n",
    "1. **Bottleneck**: 32 neurons → Hidden Layer (64 neurons)\n",
    "2. **Hidden Layer**: 64 neurons → Output Layer (50 time steps)\n",
    "3. **Final Output**: Reconstructed/denoised signal\n",
    "\n",
    "### Key Design Decisions:\n",
    "\n",
    "**Bottleneck Size (32)**: \n",
    "- Smaller than input (50) forces compression\n",
    "- Large enough to retain important information\n",
    "- Creates information bottleneck that filters out noise\n",
    "\n",
    "**ReLU Activation**:\n",
    "- Introduces non-linearity for complex pattern learning\n",
    "- Computationally efficient\n",
    "- Helps with gradient flow during training\n",
    "\n",
    "**Xavier Initialization**:\n",
    "- Prevents vanishing/exploding gradients\n",
    "- Ensures proper weight scaling\n",
    "- Improves training stability and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Autoencoder Model\n",
    "# Step 2: Define the Autoencoder Model with Initialization\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size // 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init.xavier_uniform_(module.weight)  # Xavier initialization for weights\n",
    "            if module.bias is not None:\n",
    "                init.zeros_(module.bias)  # Initialize biases to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf84608",
   "metadata": {},
   "source": [
    "## 6. Model Configuration and Training Setup\n",
    "\n",
    "### Loss Function: Mean Squared Error (MSE)\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Why MSE?**\n",
    "- **Pixel-wise comparison**: Measures point-by-point reconstruction accuracy\n",
    "- **Smooth gradients**: Provides stable training signals\n",
    "- **Noise sensitivity**: Penalizes deviations from clean signal\n",
    "- **Standard choice**: Well-established for regression and reconstruction tasks\n",
    "\n",
    "### Optimizer: Adam\n",
    "**Advantages of Adam:**\n",
    "- **Adaptive learning rates**: Automatically adjusts step sizes for each parameter\n",
    "- **Momentum**: Helps escape local minima and accelerates convergence\n",
    "- **Bias correction**: Accounts for initialization bias in early training\n",
    "- **Robust**: Works well across different problem types\n",
    "\n",
    "### Learning Rate: 0.001\n",
    "- **Conservative choice**: Prevents overshooting optimal solutions\n",
    "- **Stable training**: Reduces risk of divergence\n",
    "- **Fine-tuning**: Can be adjusted based on training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482440dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss function, and optimizer\n",
    "input_size = train_noisy.shape[1]\n",
    "hidden_size = 64\n",
    "model = DenoisingAutoencoder(input_size, hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea4cc4",
   "metadata": {},
   "source": [
    "## 7. Training Process\n",
    "\n",
    "### Training Loop Mechanics:\n",
    "\n",
    "**Batch Processing:**\n",
    "- **Batch size**: 32 sequences processed simultaneously\n",
    "- **Memory efficiency**: Balances memory usage and gradient estimation\n",
    "- **Parallel computation**: Leverages vectorized operations\n",
    "\n",
    "**Training Steps:**\n",
    "1. **Forward Pass**: Input noisy sequences → Model → Reconstructed sequences\n",
    "2. **Loss Calculation**: Compare reconstructed vs. clean sequences (MSE)\n",
    "3. **Backward Pass**: Compute gradients via backpropagation\n",
    "4. **Parameter Update**: Adjust weights using Adam optimizer\n",
    "5. **Repeat**: Continue for all batches in each epoch\n",
    "\n",
    "### Training Dynamics:\n",
    "- **1000 epochs**: Sufficient iterations for convergence\n",
    "- **Progress monitoring**: Loss printed every 10 epochs\n",
    "- **Expected behavior**: Loss should decrease and stabilize\n",
    "\n",
    "### What the Model Learns:\n",
    "- **Feature extraction**: Identifies important signal characteristics\n",
    "- **Noise patterns**: Distinguishes between signal and noise\n",
    "- **Reconstruction mapping**: Learns inverse transformation from compressed representation\n",
    "- **Generalization**: Applies learned patterns to unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train the Autoencoder\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(train_noisy), batch_size):\n",
    "        batch_noisy = train_noisy[i:i+batch_size]\n",
    "        batch_clean = train_clean[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_noisy)\n",
    "        loss = criterion(outputs, batch_clean)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8132f1",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Analysis\n",
    "\n",
    "This visualization allows us to evaluate the autoencoder's denoising performance across different test sequences.\n",
    "\n",
    "### Plot Interpretation:\n",
    "\n",
    "**Three Signal Types Displayed:**\n",
    "1. **Abnormal noisy Input (Blue)**: Original noisy measurements from sensors\n",
    "2. **Clean Sequence (Orange)**: Ground truth signal (ideal, noise-free)\n",
    "3. **Denoised Output (Green, dashed)**: Autoencoder's reconstruction\n",
    "\n",
    "### Performance Metrics to Observe:\n",
    "\n",
    "**Visual Assessment:**\n",
    "- **Signal fidelity**: How well does the denoised output match the clean signal?\n",
    "- **Noise reduction**: Is random noise effectively suppressed?\n",
    "- **Feature preservation**: Are important signal characteristics maintained?\n",
    "- **Artifact introduction**: Does the model introduce any unwanted distortions?\n",
    "\n",
    "**Expected Results:**\n",
    "- Denoised output should closely follow the clean signal\n",
    "- Random noise should be significantly reduced\n",
    "- Important signal features (peaks, trends) should be preserved\n",
    "- Smooth reconstruction without high-frequency artifacts\n",
    "\n",
    "### Real-World Implications:\n",
    "- **Equipment monitoring**: Cleaner signals enable better fault detection\n",
    "- **Control systems**: Reduced noise improves control accuracy\n",
    "- **Data analysis**: Cleaner data leads to more reliable insights\n",
    "- **Predictive maintenance**: Early anomaly detection becomes more sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb2e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize Results\n",
    "def visualize_results(model, noisy_data, clean_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(noisy_data)\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "    for i in range(3):\n",
    "        axs[i].plot(noisy_data[i].numpy(), label='Abnormal noisy Input')\n",
    "        axs[i].plot(clean_data[i].numpy(), label='Clean Sequence')\n",
    "        axs[i].plot(test_outputs[i].numpy(), label='Denoised Output', linestyle='dashed')\n",
    "        axs[i].legend()\n",
    "        axs[i].set_title(f'Sequence {i+1}')\n",
    "    \n",
    "    plt.xlabel('Time Step')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize on test data\n",
    "visualize_results(model, test_noisy, test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1bf70",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Anomaly Detection Applications\n",
    "\n",
    "### How Autoencoders Enable Anomaly Detection:\n",
    "\n",
    "**Reconstruction Error as Anomaly Indicator:**\n",
    "$$\\text{Anomaly Score} = \\|\\text{Input} - \\text{Reconstructed}\\|_2^2$$\n",
    "\n",
    "- **Normal data**: Low reconstruction error (model has seen similar patterns)\n",
    "- **Anomalous data**: High reconstruction error (model struggles to reconstruct unfamiliar patterns)\n",
    "\n",
    "### Energy System Applications:\n",
    "\n",
    "**1. Equipment Health Monitoring:**\n",
    "- **Motor vibration analysis**: Detect bearing wear, misalignment\n",
    "- **Transformer monitoring**: Identify insulation degradation, hot spots\n",
    "- **Power quality assessment**: Detect harmonics, voltage fluctuations\n",
    "\n",
    "**2. Predictive Maintenance:**\n",
    "- **Early fault detection**: Identify subtle changes before failures\n",
    "- **Maintenance scheduling**: Optimize based on actual equipment condition\n",
    "- **Cost reduction**: Prevent unexpected downtime and catastrophic failures\n",
    "\n",
    "**3. Grid Stability Analysis:**\n",
    "- **Load pattern anomalies**: Detect unusual consumption patterns\n",
    "- **Frequency deviations**: Monitor grid stability indicators\n",
    "- **Cybersecurity**: Identify potential cyber attacks on grid infrastructure\n",
    "\n",
    "### Advantages of Autoencoder Approach:\n",
    "\n",
    "**Unsupervised Learning:**\n",
    "- No need for labeled anomaly data\n",
    "- Learns normal patterns automatically\n",
    "- Adapts to changing operational conditions\n",
    "\n",
    "**Feature Learning:**\n",
    "- Automatically extracts relevant features\n",
    "- Handles high-dimensional data\n",
    "- Captures complex non-linear patterns\n",
    "\n",
    "**Scalability:**\n",
    "- Handles multiple sensors simultaneously\n",
    "- Processes real-time streaming data\n",
    "- Scales to large energy systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1dd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Exercises and Advanced Topics\n",
    "\n",
    "### Try These Modifications:\n",
    "\n",
    "**1. Architecture Experiments:**\n",
    "- **Deeper networks**: Add more layers to encoder/decoder\n",
    "- **Different activations**: Try Tanh, LeakyReLU, or Swish\n",
    "- **Regularization**: Add dropout layers or weight decay\n",
    "- **Attention mechanisms**: Implement attention-based autoencoders\n",
    "\n",
    "**2. Data Variations:**\n",
    "- **Different noise types**: Try correlated noise, periodic interference\n",
    "- **Variable sequence lengths**: Handle time series of different durations\n",
    "- **Multi-channel data**: Simulate multiple sensor inputs\n",
    "- **Real-world data**: Apply to actual energy system datasets\n",
    "\n",
    "**3. Training Improvements:**\n",
    "- **Learning rate scheduling**: Implement decay or cyclic schedules\n",
    "- **Early stopping**: Monitor validation loss to prevent overfitting\n",
    "- **Data augmentation**: Add noise variations during training\n",
    "- **Transfer learning**: Pre-train on one system, fine-tune on another\n",
    "\n",
    "**4. Anomaly Detection Extensions:**\n",
    "- **Threshold selection**: Develop statistical methods for anomaly thresholds\n",
    "- **Online detection**: Implement real-time anomaly monitoring\n",
    "- **Multivariate analysis**: Handle multiple correlated signals\n",
    "- **Temporal patterns**: Detect anomalous sequences rather than individual points\n",
    "\n",
    "### Advanced Autoencoder Variants:\n",
    "\n",
    "**Variational Autoencoders (VAE):**\n",
    "- Probabilistic approach with explicit uncertainty modeling\n",
    "- Better for generating new samples and handling uncertainty\n",
    "\n",
    "**Convolutional Autoencoders:**\n",
    "- Excellent for spatial data (images, 2D sensor arrays)\n",
    "- Preserve spatial relationships in energy system layouts\n",
    "\n",
    "**Recurrent Autoencoders (LSTM/GRU):**\n",
    "- Handle sequential dependencies in time series\n",
    "- Better for long-term temporal patterns\n",
    "\n",
    "**Transformer-based Autoencoders:**\n",
    "- State-of-the-art for sequence modeling\n",
    "- Handle long-range dependencies effectively\n",
    "\n",
    "### Performance Evaluation Metrics:\n",
    "\n",
    "**Reconstruction Quality:**\n",
    "- **MSE/RMSE**: Root mean squared error\n",
    "- **MAE**: Mean absolute error\n",
    "- **SSIM**: Structural similarity index\n",
    "- **Correlation coefficient**: Linear relationship preservation\n",
    "\n",
    "**Anomaly Detection Performance:**\n",
    "- **Precision/Recall**: Balance between false positives and detection rate\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **AUC-ROC**: Area under receiver operating characteristic curve\n",
    "- **Detection delay**: Time between anomaly occurrence and detection\n",
    "\n",
    "### Real-World Implementation Considerations:\n",
    "\n",
    "**Data Preprocessing:**\n",
    "- Normalization strategies for different signal ranges\n",
    "- Handling missing data and sensor failures\n",
    "- Time synchronization across multiple sensors\n",
    "\n",
    "**Computational Efficiency:**\n",
    "- Model compression for edge deployment\n",
    "- Quantization for faster inference\n",
    "- Batch processing for throughput optimization\n",
    "\n",
    "**Robustness:**\n",
    "- Handling concept drift in operational conditions\n",
    "- Graceful degradation with sensor failures\n",
    "- Cybersecurity considerations for industrial deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
